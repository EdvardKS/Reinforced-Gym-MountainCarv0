{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GymLibrary - Mountain_car - Reinforced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gymlibrary.dev/environments/classic_control/mountain_car/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Cómo escoger el mejor movimiento?\n",
    "## Formula de Q-Learning ->   Q(s,a)=(1−α)⋅Q(s,a)+α⋅[(R+γ⋅max a Q(s′,a))-Q(s,a)]\n",
    "<script type=\"text/javascript\" async\n",
    "  src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
    "</script>\n",
    "<body>\n",
    "  <ul>\n",
    "    <li>\n",
    "      <strong>Q(s, a):</strong>\n",
    "      Q(s,a) es el valor actual de la función Q para el estado s y la acción a.\n",
    "    </li>\n",
    "    <li>\n",
    "      <strong>α:</strong>\n",
    "      α es la tasa de aprendizaje (un valor entre 0 y 1) que controla cuánto se actualizan los valores existentes.\n",
    "    </li>\n",
    "    <li>\n",
    "      <strong>R:</strong>\n",
    "      R es la recompensa obtenida al realizar la acción a en el estado s.\n",
    "    </li>\n",
    "    <li>\n",
    "      <strong>γ:</strong>\n",
    "      γ es el factor de descuento (un valor entre 0 y 1) que representa la importancia de las recompensas futuras.\n",
    "    </li>\n",
    "    <li>\n",
    "      <strong>s′:</strong>\n",
    "      s' es el estado resultante después de realizar la acción a.\n",
    "    </li>\n",
    "    <li>\n",
    "      <strong>max(Q(s′, a)):</strong>\n",
    "      max Q(s',a) es el valor máximo de la función Q para el nuevo estado s', es decir, la mejor acción estimada para el nuevo estado.\n",
    "    </li>\n",
    "  </ul>\n",
    "</body>\n",
    "\n",
    "Nuevo_calor = Valor_actual + α [Recompensa_nueva_accion + Factor_descuento(max(Valor_Maximo_de_cualquier_accion))- Valor_actual]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym as gym\n",
    "import sys\n",
    "import pygame\n",
    "from random import randint\n",
    "env = gym.make('MountainCar-v0', render_mode=\"rgb_array\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espacio de acciones:  Discrete(3) \n",
      "Espacio de Observaciones:  Box([-1.2  -0.07], [0.6  0.07], (2,), float32) \n",
      "Posiciones Iniciales entre -0.4 y 0.6:  (array([-0.48343515,  0.        ], dtype=float32), {}) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Espacio de acciones: \",env.action_space,\"\\nEspacio de Observaciones: \",env.observation_space,\"\\nPosiciones Iniciales entre -0.4 y 0.6: \",env.reset(),\"\\n\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretizar(valor):\n",
    "    aux = ((valor-env.observation_space.low) / (env.observation_space.high-env.observation_space.low)) *20\n",
    "    return tuple(aux.astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.42705727,  0.        ], dtype=float32), {})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discretizar(env.reset()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.random.uniform(low=-1,high=1,size=[20,20,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabla Q-Learning\n",
    "\n",
    "No pretendamos meter todas las posiciones en la tabla, Siendo más simples, solo vamos a coger todos los valores y los pasamos a discretos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio:  1 \n",
      "Media de las recompensas:  -2529.0 \n",
      "\n",
      "Episodio:  100 \n",
      "Media de las recompensas:  -1349.13 \n",
      "\n",
      "Episodio:  200 \n",
      "Media de las recompensas:  -1396.31 \n",
      "\n",
      "Episodio:  300 \n",
      "Media de las recompensas:  -1423.8333333333333 \n",
      "\n",
      "Episodio:  400 \n",
      "Media de las recompensas:  -1448.83 \n",
      "\n",
      "Episodio:  500 \n",
      "Media de las recompensas:  -1444.918 \n",
      "\n",
      "Episodio:  600 \n",
      "Media de las recompensas:  -1450.8466666666666 \n",
      "\n",
      "Episodio:  700 \n",
      "Media de las recompensas:  -1472.1585714285713 \n",
      "\n",
      "Episodio:  800 \n",
      "Media de las recompensas:  -1468.21625 \n",
      "\n",
      "Episodio:  900 \n",
      "Media de las recompensas:  -1441.3155555555556 \n",
      "\n",
      "Episodio:  1000 \n",
      "Media de las recompensas:  -1448.356 \n",
      "\n",
      "Episodio:  1100 \n",
      "Media de las recompensas:  -1451.6954545454546 \n",
      "\n",
      "Episodio:  1200 \n",
      "Media de las recompensas:  -1451.7325 \n",
      "\n",
      "Episodio:  1300 \n",
      "Media de las recompensas:  -1438.5653846153846 \n",
      "\n",
      "Episodio:  1400 \n",
      "Media de las recompensas:  -1443.9092857142857 \n",
      "\n",
      "Episodio:  1500 \n",
      "Media de las recompensas:  -1462.9686666666666 \n",
      "\n",
      "Episodio:  1600 \n",
      "Media de las recompensas:  -1466.975625 \n",
      "\n",
      "Episodio:  1700 \n",
      "Media de las recompensas:  -1462.7258823529412 \n",
      "\n",
      "Episodio:  1800 \n",
      "Media de las recompensas:  -1464.0827777777777 \n",
      "\n",
      "Episodio:  1900 \n",
      "Media de las recompensas:  -1453.3047368421053 \n",
      "\n",
      "Episodio:  2000 \n",
      "Media de las recompensas:  -1455.176 \n",
      "\n",
      "Episodio:  2100 \n",
      "Media de las recompensas:  -1456.5433333333333 \n",
      "\n",
      "Episodio:  2200 \n",
      "Media de las recompensas:  -1454.9104545454545 \n",
      "\n",
      "Episodio:  2300 \n",
      "Media de las recompensas:  -1455.3830434782608 \n",
      "\n",
      "Episodio:  2400 \n",
      "Media de las recompensas:  -1457.76375 \n",
      "\n",
      "Episodio:  2500 \n",
      "Media de las recompensas:  -1456.1752 \n",
      "\n",
      "Episodio:  2600 \n",
      "Media de las recompensas:  -1453.9538461538461 \n",
      "\n",
      "Episodio:  2700 \n",
      "Media de las recompensas:  -1458.9996296296297 \n",
      "\n",
      "Episodio:  2800 \n",
      "Media de las recompensas:  -1458.4567857142856 \n",
      "\n",
      "Episodio:  2900 \n",
      "Media de las recompensas:  -1462.7824137931034 \n",
      "\n",
      "Episodio:  3000 \n",
      "Media de las recompensas:  -1462.116 \n",
      "\n",
      "Episodio:  3100 \n",
      "Media de las recompensas:  -1465.5532258064516 \n",
      "\n",
      "Episodio:  3200 \n",
      "Media de las recompensas:  -1460.0484375 \n",
      "\n",
      "Episodio:  3300 \n",
      "Media de las recompensas:  -1461.1457575757577 \n",
      "\n",
      "Episodio:  3400 \n",
      "Media de las recompensas:  -1466.1826470588235 \n",
      "\n",
      "Episodio:  3500 \n",
      "Media de las recompensas:  -1468.142 \n",
      "\n",
      "Episodio:  3600 \n",
      "Media de las recompensas:  -1471.4716666666666 \n",
      "\n",
      "Episodio:  3700 \n",
      "Media de las recompensas:  -1469.2613513513513 \n",
      "\n",
      "Episodio:  3800 \n",
      "Media de las recompensas:  -1465.311052631579 \n",
      "\n",
      "Episodio:  3900 \n",
      "Media de las recompensas:  -1461.0792307692307 \n",
      "\n",
      "Episodio:  4000 \n",
      "Media de las recompensas:  -1462.73675 \n",
      "\n",
      "Episodio:  4100 \n",
      "Media de las recompensas:  -1464.8868292682928 \n",
      "\n",
      "Episodio:  4200 \n",
      "Media de las recompensas:  -1464.8642857142856 \n",
      "\n",
      "Episodio:  4300 \n",
      "Media de las recompensas:  -1467.9541860465117 \n",
      "\n",
      "Episodio:  4400 \n",
      "Media de las recompensas:  -1466.4309090909092 \n",
      "\n",
      "Episodio:  4500 \n",
      "Media de las recompensas:  -1467.0468888888888 \n",
      "\n",
      "Episodio:  4600 \n",
      "Media de las recompensas:  -1465.442608695652 \n",
      "\n",
      "Episodio:  4700 \n",
      "Media de las recompensas:  -1463.3055319148937 \n",
      "\n",
      "Episodio:  4800 \n",
      "Media de las recompensas:  -1464.96875 \n",
      "\n",
      "Episodio:  4900 \n",
      "Media de las recompensas:  -1465.5277551020408 \n",
      "\n",
      "Episodio:  5000 \n",
      "Media de las recompensas:  -1466.4424 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pygame.init()\n",
    "ventana = pygame.display.set_mode((600, 400))\n",
    "learning_rate = 0.1\n",
    "factor_descuento = 0.95\n",
    "listado_recompensas = []\n",
    "\n",
    "for i in range(5000):\n",
    "    # print(listado_recompensas)\n",
    "    # print(i+1)\n",
    "    estado = discretizar(env.reset()[0])\n",
    "    final = False   \n",
    "    recompensa_total = 0\n",
    "    while not final:\n",
    "\n",
    "        if randint(0,10)>2: # El 20% de las veces que coja el mejor estado\n",
    "            accion = np.argmax(q_table[estado])\n",
    "        else:\n",
    "            accion = randint(0,2)\n",
    "\n",
    "\n",
    "        nuevo_estado, recompensa, final, truncated, info  = env.step(accion)\n",
    "        q_table[estado][accion] = q_table[estado][accion] + learning_rate * (recompensa + factor_descuento * np.max(q_table[discretizar(nuevo_estado)]) - q_table[estado][accion])\n",
    "\n",
    "        recompensa_total += recompensa\n",
    "        if (i+1) % 500 == 0 or i == 0:\n",
    "            superficie = pygame.Surface((env.render().shape[1], env.render().shape[0]))\n",
    "            pygame.surfarray.blit_array(superficie, np.transpose(env.render(), (1, 0, 2)).astype(np.uint8))\n",
    "\n",
    "            try:\n",
    "                ventana.blit(superficie, (0, 0))\n",
    "                pygame.display.update()\n",
    "            except pygame.error as e:\n",
    "                print(\"Error al actualizar la ventana de pygame:\", e)\n",
    "\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n",
    "    \n",
    "    listado_recompensas.append(recompensa_total)\n",
    "\n",
    "    if (i+1) % 100 == 0 or i == 0:\n",
    "        print(\"Episodio: \",i+1, \"\\nMedia de las recompensas: \",np.mean(listado_recompensas),\"\\n\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En el siguiente Modelo cambiamos el porcentaje para que coja el mejore estado, y le hemos asignado que el 80% de los casos coja el mejor de estos casos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio:  1 \n",
      "Media de las recompensas:  -6245.0 \n",
      "\n",
      "Episodio:  100 \n",
      "Media de las recompensas:  -13325.27 \n",
      "\n",
      "Episodio:  200 \n",
      "Media de las recompensas:  -12044.945 \n",
      "\n",
      "Episodio:  300 \n",
      "Media de las recompensas:  -13009.356666666667 \n",
      "\n",
      "Episodio:  400 \n",
      "Media de las recompensas:  -12724.4675 \n",
      "\n",
      "Episodio:  500 \n",
      "Media de las recompensas:  -12333.532 \n",
      "\n",
      "Episodio:  600 \n",
      "Media de las recompensas:  -12901.17 \n",
      "\n",
      "Episodio:  700 \n",
      "Media de las recompensas:  -13358.931428571428 \n",
      "\n",
      "Episodio:  800 \n",
      "Media de las recompensas:  -13235.68875 \n",
      "\n",
      "Episodio:  900 \n",
      "Media de las recompensas:  -13333.452222222222 \n",
      "\n",
      "Episodio:  1000 \n",
      "Media de las recompensas:  -13194.602 \n",
      "\n",
      "Episodio:  1100 \n",
      "Media de las recompensas:  -13083.658181818182 \n",
      "\n",
      "Episodio:  1200 \n",
      "Media de las recompensas:  -13025.566666666668 \n",
      "\n",
      "Episodio:  1300 \n",
      "Media de las recompensas:  -13135.88153846154 \n",
      "\n",
      "Episodio:  1400 \n",
      "Media de las recompensas:  -13037.021428571428 \n",
      "\n",
      "Episodio:  1500 \n",
      "Media de las recompensas:  -12999.368 \n",
      "\n",
      "Episodio:  1600 \n",
      "Media de las recompensas:  -12994.43625 \n",
      "\n",
      "Episodio:  1700 \n",
      "Media de las recompensas:  -12938.220588235294 \n",
      "\n",
      "Episodio:  1800 \n",
      "Media de las recompensas:  -12998.923888888889 \n",
      "\n",
      "Episodio:  1900 \n",
      "Media de las recompensas:  -13101.28947368421 \n",
      "\n",
      "Episodio:  2000 \n",
      "Media de las recompensas:  -13140.1435 \n",
      "\n",
      "Episodio:  2100 \n",
      "Media de las recompensas:  -13242.996666666666 \n",
      "\n",
      "Episodio:  2200 \n",
      "Media de las recompensas:  -13230.211363636363 \n",
      "\n",
      "Episodio:  2300 \n",
      "Media de las recompensas:  -13271.030869565217 \n",
      "\n",
      "Episodio:  2400 \n",
      "Media de las recompensas:  -13172.0525 \n",
      "\n",
      "Episodio:  2500 \n",
      "Media de las recompensas:  -13152.1744 \n",
      "\n",
      "Episodio:  2600 \n",
      "Media de las recompensas:  -13188.558846153846 \n",
      "\n",
      "Episodio:  2700 \n",
      "Media de las recompensas:  -13235.147407407407 \n",
      "\n",
      "Episodio:  2800 \n",
      "Media de las recompensas:  -13297.182857142858 \n",
      "\n",
      "Episodio:  2900 \n",
      "Media de las recompensas:  -13260.05448275862 \n",
      "\n",
      "Episodio:  3000 \n",
      "Media de las recompensas:  -13221.402 \n",
      "\n",
      "Episodio:  3100 \n",
      "Media de las recompensas:  -13222.740967741935 \n",
      "\n",
      "Episodio:  3200 \n",
      "Media de las recompensas:  -13208.9684375 \n",
      "\n",
      "Episodio:  3300 \n",
      "Media de las recompensas:  -13163.23909090909 \n",
      "\n",
      "Episodio:  3400 \n",
      "Media de las recompensas:  -13161.705294117646 \n",
      "\n",
      "Episodio:  3500 \n",
      "Media de las recompensas:  -13126.793142857143 \n",
      "\n",
      "Episodio:  3600 \n",
      "Media de las recompensas:  -13162.827222222222 \n",
      "\n",
      "Episodio:  3700 \n",
      "Media de las recompensas:  -13144.157837837838 \n",
      "\n",
      "Episodio:  3800 \n",
      "Media de las recompensas:  -13120.922368421052 \n",
      "\n",
      "Episodio:  3900 \n",
      "Media de las recompensas:  -13117.922564102564 \n",
      "\n",
      "Episodio:  4000 \n",
      "Media de las recompensas:  -13113.1265 \n",
      "\n",
      "Episodio:  4100 \n",
      "Media de las recompensas:  -13129.274390243903 \n",
      "\n",
      "Episodio:  4200 \n",
      "Media de las recompensas:  -13170.357142857143 \n",
      "\n",
      "Episodio:  4300 \n",
      "Media de las recompensas:  -13168.153720930233 \n",
      "\n",
      "Episodio:  4400 \n",
      "Media de las recompensas:  -13151.415909090909 \n",
      "\n",
      "Episodio:  4500 \n",
      "Media de las recompensas:  -13145.392 \n",
      "\n",
      "Episodio:  4600 \n",
      "Media de las recompensas:  -13148.808695652175 \n",
      "\n",
      "Episodio:  4700 \n",
      "Media de las recompensas:  -13120.367659574467 \n",
      "\n",
      "Episodio:  4800 \n",
      "Media de las recompensas:  -13096.06375 \n",
      "\n",
      "Episodio:  4900 \n",
      "Media de las recompensas:  -13126.380816326531 \n",
      "\n",
      "Episodio:  5000 \n",
      "Media de las recompensas:  -13143.281 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pygame.init()\n",
    "ventana = pygame.display.set_mode((600, 400))\n",
    "learning_rate = 0.1\n",
    "factor_descuento = 0.95\n",
    "listado_recompensas = []\n",
    "\n",
    "for i in range(5000):\n",
    "    # print(listado_recompensas)\n",
    "    # print(i+1)\n",
    "    estado = discretizar(env.reset()[0])\n",
    "    final = False   \n",
    "    recompensa_total = 0\n",
    "    while not final:\n",
    "\n",
    "        if randint(0,10)>8: # El 80% de las veces que coja el mejor estado\n",
    "            accion = np.argmax(q_table[estado])\n",
    "        else:\n",
    "            accion = randint(0,2)\n",
    "\n",
    "\n",
    "        nuevo_estado, recompensa, final, truncated, info  = env.step(accion)\n",
    "        q_table[estado][accion] = q_table[estado][accion] + learning_rate * (recompensa + factor_descuento * np.max(q_table[discretizar(nuevo_estado)]) - q_table[estado][accion])\n",
    "\n",
    "        recompensa_total += recompensa\n",
    "        if (i+1) % 500 == 0 or i == 0:\n",
    "            superficie = pygame.Surface((env.render().shape[1], env.render().shape[0]))\n",
    "            pygame.surfarray.blit_array(superficie, np.transpose(env.render(), (1, 0, 2)).astype(np.uint8))\n",
    "\n",
    "            try:\n",
    "                ventana.blit(superficie, (0, 0))\n",
    "                pygame.display.update()\n",
    "            except pygame.error as e:\n",
    "                print(\"Error al actualizar la ventana de pygame:\", e)\n",
    "\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n",
    "    \n",
    "    listado_recompensas.append(recompensa_total)\n",
    "\n",
    "    if (i+1) % 100 == 0 or i == 0:\n",
    "        print(\"Episodio: \",i+1, \"\\nMedia de las recompensas: \",np.mean(listado_recompensas),\"\\n\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora implantaré un modelo que entrene y se guarde sus puntuaciones, y las proximas veces que juegue, que sea consultando, de esta manera podremos Mejorar La puntuación progresivamente mientras más juegue nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cargo y Leo toda la q_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_politica():\n",
    "    if exists(\"politica.pkl\"):\n",
    "        with open(\"politica.pkl\", 'rb') as fr:\n",
    "            q_table = pickle.load(fr)\n",
    "        return q_table\n",
    "    else:\n",
    "        with open(\"politica.pkl\", 'wb') as fw:\n",
    "            q_table_sumada = np.random.uniform(low=-1,high=1,size=[20,20,3])\n",
    "            pickle.dump(q_table_sumada, fw)\n",
    "        return q_table_sumada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_politica(q_table):\n",
    "    politicas_existente = cargar_politica()  # Cargar las políticas existentes\n",
    "    if politicas_existente is not None:\n",
    "        # Sumar las nuevas q_table a las anteriores\n",
    "        q_table_sumada = politicas_existente + q_table\n",
    "    else:\n",
    "        # Si no hay políticas existentes, usar la q_table actual\n",
    "        q_table_sumada = q_table\n",
    "    \n",
    "    with open(\"politica.pkl\", 'wb') as fw:\n",
    "        pickle.dump(q_table_sumada, fw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio:  1 \n",
      "Media de las recompensas:  -8056.0 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m nuevo_estado, recompensa, final, truncated, info  \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(accion)\n\u001b[0;32m     25\u001b[0m q_table[estado][accion] \u001b[38;5;241m=\u001b[39m q_table[estado][accion] \u001b[38;5;241m+\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m (recompensa \u001b[38;5;241m+\u001b[39m factor_descuento \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(q_table[discretizar(nuevo_estado)]) \u001b[38;5;241m-\u001b[39m q_table[estado][accion])\n\u001b[1;32m---> 27\u001b[0m \u001b[43mguardar_politica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m recompensa_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m recompensa\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# Para que el entreno no se nos haga muy largo, indicamos ver el archivo en 0 y cada 2500\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[63], line 10\u001b[0m, in \u001b[0;36mguardar_politica\u001b[1;34m(q_table)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Si no hay políticas existentes, usar la q_table actual\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     q_table_sumada \u001b[38;5;241m=\u001b[39m q_table\n\u001b[1;32m---> 10\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpolitica.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfw\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_table_sumada\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pygame.init()\n",
    "ventana = pygame.display.set_mode((600, 400))\n",
    "learning_rate = 0.1\n",
    "factor_descuento = 0.95\n",
    "listado_recompensas = []\n",
    "\n",
    "for i in range(5000):\n",
    "    # print(listado_recompensas)\n",
    "    # print(i+1)\n",
    "    estado = discretizar(env.reset()[0])\n",
    "    final = False   \n",
    "    recompensa_total = 0\n",
    "    while not final:\n",
    "        # Indicamos la primera que and i != 0, pq si no tenemos el archivo creado primero debe acceder al randint y en el Guardado ya tendremos algún resultado para consultar dentro del if()\n",
    "        if randint(0,10)>5 and i != 0: # El 50% de las veces que coja el mejor estado - Esto proporcionará que el modelo explore otros resultados el 50% de las ocasiones.\n",
    "            politicas = cargar_politica() # Cargaremos las Politicas que ya se han guardado alguna vez, y si no existen las creamos.\n",
    "            # Las políticas deberían ser... todas las q_table anteriores.\n",
    "            # accion = np.argmax(q_table[estado]) # Entonces aquí ya no consultamos q_table[estado], si no politicas[estado]\n",
    "            accion = np.argmax(politicas[estado])\n",
    "        else:\n",
    "            accion = randint(0,2)\n",
    "\n",
    "\n",
    "        nuevo_estado, recompensa, final, truncated, info  = env.step(accion)\n",
    "        q_table[estado][accion] = q_table[estado][accion] + learning_rate * (recompensa + factor_descuento * np.max(q_table[discretizar(nuevo_estado)]) - q_table[estado][accion])\n",
    "        \n",
    "        guardar_politica(q_table)\n",
    "\n",
    "        recompensa_total += recompensa\n",
    "        if (i+1) % 2500 == 0 or i == 0: # Para que el entreno no se nos haga muy largo, indicamos ver el archivo en 0 y cada 2500\n",
    "            ### Cuando i es 2499: (2499 + 1) % 2500 == 0\n",
    "            ### Cuando i es 4999: (4999 + 1) % 2500 == 0\n",
    "            superficie = pygame.Surface((env.render().shape[1], env.render().shape[0]))\n",
    "            pygame.surfarray.blit_array(superficie, np.transpose(env.render(), (1, 0, 2)).astype(np.uint8))\n",
    "\n",
    "            try:\n",
    "                ventana.blit(superficie, (0, 0))\n",
    "                pygame.display.update()\n",
    "            except pygame.error as e:\n",
    "                print(\"Error al actualizar la ventana de pygame:\", e)\n",
    "\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n",
    "    \n",
    "    listado_recompensas.append(recompensa_total)\n",
    "\n",
    "    if (i+1) % 100 == 0 or i == 0:\n",
    "        print(\"Episodio: \",i+1, \"\\nMedia de las recompensas: \",np.mean(listado_recompensas),\"\\n\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancelo el entrenamiento porqué es muy lento, en 10 minutos no ha llegado al entreno número 100\n",
    "\n",
    "## Voy a probar con cargar y guardar los datos solo al principio y final de cada partida manteniendolos en memoria RAM, a ver si de esta manera es más rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q_table = np.random.uniform(low=-1,high=1,size=[20,20,3])\n",
    "q_table = cargar_politica()\n",
    "len(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio:  1 \n",
      "Media de las recompensas:  -1.4540908416958866 \n",
      "\n",
      "Episodio:  100 \n",
      "Media de las recompensas:  -1.4224094987473885 \n",
      "\n",
      "Episodio:  200 \n",
      "Media de las recompensas:  -1.4187551569684758 \n",
      "\n",
      "Episodio:  300 \n",
      "Media de las recompensas:  -1.4282234571533452 \n",
      "\n",
      "Episodio:  400 \n",
      "Media de las recompensas:  -1.4167289871391455 \n",
      "\n",
      "Episodio:  500 \n",
      "Media de las recompensas:  -1.4224242873079234 \n",
      "\n",
      "Episodio:  600 \n",
      "Media de las recompensas:  -1.4184043738636885 \n",
      "\n",
      "Episodio:  700 \n",
      "Media de las recompensas:  -1.4192903251960138 \n",
      "\n",
      "Episodio:  800 \n",
      "Media de las recompensas:  -1.4181152461874205 \n",
      "\n",
      "Episodio:  900 \n",
      "Media de las recompensas:  -1.4222775913141688 \n",
      "\n",
      "Episodio:  1000 \n",
      "Media de las recompensas:  -1.4303242833502536 \n",
      "\n",
      "Episodio:  1100 \n",
      "Media de las recompensas:  -1.434301814947985 \n",
      "\n",
      "Episodio:  1200 \n",
      "Media de las recompensas:  -1.4406396112011448 \n",
      "\n",
      "Episodio:  1300 \n",
      "Media de las recompensas:  -1.4410531266561681 \n",
      "\n",
      "Episodio:  1400 \n",
      "Media de las recompensas:  -1.4386598818643785 \n",
      "\n",
      "Episodio:  1500 \n",
      "Media de las recompensas:  -1.438563374345833 \n",
      "\n",
      "Episodio:  1600 \n",
      "Media de las recompensas:  -1.436540160736138 \n",
      "\n",
      "Episodio:  1700 \n",
      "Media de las recompensas:  -1.437714904568812 \n",
      "\n",
      "Episodio:  1800 \n",
      "Media de las recompensas:  -1.4368180466819902 \n",
      "\n",
      "Episodio:  1900 \n",
      "Media de las recompensas:  -1.4343447922363488 \n",
      "\n",
      "Episodio:  2000 \n",
      "Media de las recompensas:  -1.434448815144004 \n",
      "\n",
      "Episodio:  2100 \n",
      "Media de las recompensas:  -1.4351149793468436 \n",
      "\n",
      "Episodio:  2200 \n",
      "Media de las recompensas:  -1.4364097955650872 \n",
      "\n",
      "Episodio:  2300 \n",
      "Media de las recompensas:  -1.4378980701742416 \n",
      "\n",
      "Episodio:  2400 \n",
      "Media de las recompensas:  -1.4373732820371674 \n",
      "\n",
      "Episodio:  2500 \n",
      "Media de las recompensas:  -1.436285707658808 \n",
      "\n",
      "Episodio:  2600 \n",
      "Media de las recompensas:  -1.4352022157810336 \n",
      "\n",
      "Episodio:  2700 \n",
      "Media de las recompensas:  -1.4355774878390626 \n",
      "\n",
      "Episodio:  2800 \n",
      "Media de las recompensas:  -1.4340281645277417 \n",
      "\n",
      "Episodio:  2900 \n",
      "Media de las recompensas:  -1.4331194052842322 \n",
      "\n",
      "Episodio:  3000 \n",
      "Media de las recompensas:  -1.4322747449908304 \n",
      "\n",
      "Episodio:  3100 \n",
      "Media de las recompensas:  -1.4321199366244892 \n",
      "\n",
      "Episodio:  3200 \n",
      "Media de las recompensas:  -1.4334022926602572 \n",
      "\n",
      "Episodio:  3300 \n",
      "Media de las recompensas:  -1.4328964902865238 \n",
      "\n",
      "Episodio:  3400 \n",
      "Media de las recompensas:  -1.4336290367950788 \n",
      "\n",
      "Episodio:  3500 \n",
      "Media de las recompensas:  -1.4348734044144773 \n",
      "\n",
      "Episodio:  3600 \n",
      "Media de las recompensas:  -1.433857089175492 \n",
      "\n",
      "Episodio:  3700 \n",
      "Media de las recompensas:  -1.4340844633433891 \n",
      "\n",
      "Episodio:  3800 \n",
      "Media de las recompensas:  -1.4345104006059115 \n",
      "\n",
      "Episodio:  3900 \n",
      "Media de las recompensas:  -1.4333595510458603 \n",
      "\n",
      "Episodio:  4000 \n",
      "Media de las recompensas:  -1.4341856888138396 \n",
      "\n",
      "Episodio:  4100 \n",
      "Media de las recompensas:  -1.432944190128952 \n",
      "\n",
      "Episodio:  4200 \n",
      "Media de las recompensas:  -1.434072103390093 \n",
      "\n",
      "Episodio:  4300 \n",
      "Media de las recompensas:  -1.4342889631988756 \n",
      "\n",
      "Episodio:  4400 \n",
      "Media de las recompensas:  -1.4332670826780396 \n",
      "\n",
      "Episodio:  4500 \n",
      "Media de las recompensas:  -1.433659139799516 \n",
      "\n",
      "Episodio:  4600 \n",
      "Media de las recompensas:  -1.4339749866277895 \n",
      "\n",
      "Episodio:  4700 \n",
      "Media de las recompensas:  -1.4350050182330922 \n",
      "\n",
      "Episodio:  4800 \n",
      "Media de las recompensas:  -1.4345734022379188 \n",
      "\n",
      "Episodio:  4900 \n",
      "Media de las recompensas:  -1.4345196630904944 \n",
      "\n",
      "Episodio:  5000 \n",
      "Media de las recompensas:  -1.4344879009634257 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pygame.init()\n",
    "ventana = pygame.display.set_mode((600, 400))\n",
    "learning_rate = 0.2                               # Aquí ya me encuentro modificando en learning_rate y el factor_descuento para observar cambios\n",
    "factor_descuento = 0.95\n",
    "listado_recompensas = []\n",
    "\n",
    "for i in range(5000):\n",
    "    estado = discretizar(env.reset()[0])\n",
    "    final = False   \n",
    "    recompensa_total = 0\n",
    "    while not final:\n",
    "        # Indicamos la primera que and i != 0, pq si no tenemos el archivo creado primero debe acceder al randint y en el Guardado ya tendremos algún resultado para consultar dentro del if()\n",
    "        if randint(0,10)>2 and i != 0:\n",
    "            accion = np.argmax(q_table[estado])\n",
    "        else:\n",
    "            accion = randint(0,2)\n",
    "\n",
    "\n",
    "        nuevo_estado, recompensa, final, truncated, info  = env.step(accion)\n",
    "        # q_table[estado][accion] = q_table[estado][accion] + learning_rate * (recompensa + factor_descuento * np.max(q_table[discretizar(nuevo_estado)]) - q_table[estado][accion])\n",
    "\n",
    "                # Actualiza la política solo si la recompensa es mejor\n",
    "        nueva_recompensa = recompensa + factor_descuento * np.max(q_table[discretizar(nuevo_estado)])\n",
    "        if nueva_recompensa > recompensa_total:\n",
    "            q_table[estado][accion] = q_table[estado][accion] + learning_rate * (recompensa + factor_descuento * np.max(q_table[discretizar(nuevo_estado)]) - q_table[estado][accion])\n",
    "            recompensa_total = nueva_recompensa\n",
    "\n",
    "\n",
    "        recompensa_total += recompensa\n",
    "        if (i+1) % 2500 == 0 or i == 0: # Para que el entreno no se nos haga muy largo, indicamos ver el archivo en 0 y cada 2500\n",
    "            ### Cuando i es 2499: (2499 + 1) % 2500 == 0\n",
    "            ### Cuando i es 4999: (4999 + 1) % 2500 == 0\n",
    "            superficie = pygame.Surface((env.render().shape[1], env.render().shape[0]))\n",
    "            pygame.surfarray.blit_array(superficie, np.transpose(env.render(), (1, 0, 2)).astype(np.uint8))\n",
    "\n",
    "            try:\n",
    "                ventana.blit(superficie, (0, 0))\n",
    "                pygame.display.update()\n",
    "            except pygame.error as e:\n",
    "                print(\"Error al actualizar la ventana de pygame:\", e)\n",
    "\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n",
    "    \n",
    "    listado_recompensas.append(recompensa_total)\n",
    "\n",
    "    if (i+1) % 100 == 0 or i == 0:\n",
    "        print(\"Episodio: \",i+1, \"\\nMedia de las recompensas: \",np.mean(listado_recompensas),\"\\n\")\n",
    "        \n",
    "guardar_politica(q_table)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observamos un cmabio rádical! El porcentaje de Error es mucho menor.\n",
    "\n",
    "## Al fin y al cambo añadimos que solo se guarde en el archivo de q_table, los mejores movimientos, y de esta manera obtenemos el mejor resultado enseguida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
